ifndef::imagesdir[:imagesdir: ../images]

[[section-deployment-view]]


== Deployment View

ifdef::arc42help[]
[role="arc42help"]
****
.Content
The deployment view describes:

 1. technical infrastructure used to execute your system, with infrastructure elements like geographical locations, environments, computers, processors, channels and net topologies as well as other infrastructure elements and

2. mapping of (software) building blocks to that infrastructure elements.

Often systems are executed in different environments, e.g. development environment, test environment, production environment. In such cases you should document all relevant environments.

Especially document a deployment view if your software is executed as distributed system with more than one computer, processor, server or container or when you design and construct your own hardware processors and chips.

From a software perspective it is sufficient to capture only those elements of an infrastructure that are needed to show a deployment of your building blocks. Hardware architects can go beyond that and describe an infrastructure to any level of detail they need to capture.

.Motivation
Software does not run without hardware.
This underlying infrastructure can and will influence a system and/or some
cross-cutting concepts. Therefore, there is a need to know the infrastructure.

.Form

Maybe a highest level deployment diagram is already contained in section 3.2. as
technical context with your own infrastructure as ONE black box. In this section one can
zoom into this black box using additional deployment diagrams:

* UML offers deployment diagrams to express that view. Use it, probably with nested diagrams,
when your infrastructure is more complex.
* When your (hardware) stakeholders prefer other kinds of diagrams rather than a deployment diagram, let them use any kind that is able to show nodes and channels of the infrastructure.


.Further Information

See https://docs.arc42.org/section-7/[Deployment View] in the arc42 documentation.

****
endif::arc42help[]

=== Infrastructure Level 1

ifdef::arc42help[]
[role="arc42help"]
****
Describe (usually in a combination of diagrams, tables, and text):

* distribution of a system to multiple locations, environments, computers, processors, .., as well as physical connections between them
* important justifications or motivations for this deployment structure
* quality and/or performance features of this infrastructure
* mapping of software artifacts to elements of this infrastructure

For multiple environments or alternative deployments please copy and adapt this section of arc42 for all relevant environments.
****
endif::arc42help[]

_**Overview Diagram**_

[plantuml, "deployment-level1", png]
----
@startuml
title Deployment View - Yovi (Localhost)

skinparam shadowing false
skinparam componentStyle rectangle

actor User

node "Client" as client {
  [Terminal (User agent)] as terminal
}

node "Docker Host (Localhost)" as host {
  frame "monitor-net (Docker Network)" as net {
    [WebApp] as webapp
    [Users Service] as users
    [GameY Service] as gamey
    [Prometheus] as prom
    [Grafana] as graf
    database "MongoDB" as mongo
  }
}

User --> terminal : Executes commands\n(manual input)

terminal --> webapp : Sends requests\n(JSON)
webapp --> users : Sends API requests\n(JSON)
webapp --> gamey : Sends API requests\n(JSON)

users --> mongo : Read/Write data
gamey --> mongo : Read/Write data

graf --> prom : Queries data
prom --> users : Collects metrics
prom --> gamey : Collects metrics
@enduml
----

Motivation::

Yovi is executed locally on a single host machine using Docker and Docker Compose. The system is deployed as a set of containers connected through a private Docker network ("monitor-net"). This deployment shows where each container runs and the main communication paths between them, including the monitoring stack (Prometheus and Grafana).

Quality and/or Performance Features::

* Modularity: each service is deployed as an independent container, allowing isolated development and replacement.
* Portability: the deployment can be reproduced consistently on different machines that support Docker.
* Observability: Prometheus and Grafana enable basic monitoring of the backend services.
* Maintainability: the separation of responsibilities across containers simplifies troubleshooting and updates.

Mapping of Building Blocks to Infrastructure::
[cols="1,2", options="header"]
|===
|Building Block |Infrastructure Element

|Terminal (User agent)
|Runs on the client side and is used to execute commands that interact with the system.

|WebApp
|Container deployed on the Docker Host, connected to the internal Docker network.

|Users Service
|Container deployed on the Docker Host, connected to the internal Docker network.

|GameY Service
|Container deployed on the Docker Host, connected to the internal Docker network.

|MongoDB
|Container deployed on the Docker Host, connected to the internal Docker network.

|Prometheus
|Container deployed on the Docker Host, connected to the internal Docker network. Collects metrics from the services.

|Grafana
|Container deployed on the Docker Host, connected to the internal Docker network. Visualizes metrics queried from Prometheus.
|===


=== Infrastructure Level 2

ifdef::arc42help[]
[role="arc42help"]
****
Here you can include the internal structure of (some) infrastructure elements from level 1.

Please copy the structure from level 1 for each selected element.
****
endif::arc42help[]

==== _Docker Host (Localhost)_

[plantuml, "deployment-level2-host", png]
----
@startuml
title Infrastructure Level 2 - Docker Host (Localhost)

skinparam shadowing false
skinparam componentStyle rectangle

node "Docker Host (Localhost)" as host {
  frame "monitor-net (Docker Network)" as net {
    [WebApp] as webapp
    [Users Service] as users
    [GameY Service] as gamey
    database "MongoDB" as mongo
    [Prometheus] as prom
    [Grafana] as graf
  }
}

webapp --> users : Requests/Responses\n(JSON)
webapp --> gamey : Requests/Responses\n(JSON)

users --> mongo : Persist user data
gamey --> mongo : Persist game data (if used)

prom --> users : Metrics data
prom --> gamey : Metrics data
graf --> prom : Dashboard queries
@enduml
----

This diagram zooms into the Docker Host and the internal Docker network. All components are deployed as containers within the same private network, allowing direct communication between services. WebApp interacts with backend services (Users Service and GameY Service) using JSON messages. MongoDB stores persistent data. Prometheus and Grafana run alongside the services to provide basic monitoring and visualization.

==== _Client (Terminal)_

[plantuml, "deployment-level2-client", png]
----
@startuml
title Infrastructure Level 2 - Client (Terminal)

skinparam shadowing false
skinparam componentStyle rectangle

actor "User" as user

node "Client" as client {
  [Terminal (User agent)] as terminal
}

node "Docker Host (Localhost)" as host {
  [WebApp] as webapp
}

user --> terminal : Executes commands
terminal --> webapp : Sends requests\n(JSON)
@enduml
----

This diagram isolates the client-side element used during development/testing. The user executes commands via a terminal. Those commands generate requests toward the WebApp, which acts as the entry point to the deployed system.

==== _Monitoring Stack (Prometheus + Grafana)_

[plantuml, "deployment-level2-monitoring", png]
----
@startuml
title Infrastructure Level 2 - Monitoring Stack

skinparam shadowing false
skinparam componentStyle rectangle

node "Docker Host (Localhost)" as host {
  frame "monitor-net (Docker Network)" as net {
    [Users Service] as users
    [GameY Service] as gamey
    [Prometheus] as prom
    [Grafana] as graf
  }
}

prom --> users : Collects metrics
prom --> gamey : Collects metrics
graf --> prom : Queries metrics
@enduml
----

This diagram focuses on observability. Prometheus collects technical metrics exposed by the backend services and stores them internally. Grafana queries Prometheus to display dashboards. This setup supports troubleshooting and performance monitoring during development.
